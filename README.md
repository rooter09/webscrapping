# Product Data Explorer - Full-Stack Web Scraping Application

A production-ready product exploration platform that enables users to navigate from high-level headings ‚Üí categories ‚Üí products ‚Üí product detail pages, powered by live, on-demand scraping from **World of Books**.

## üöÄ Live Demo

- **Frontend**: [Deployment URL - To be added after deployment]
- **Backend API**: [Deployment URL - To be added after deployment]
- **API Documentation**: [Swagger URL - To be added after deployment]

## üìã Table of Contents

- [Features](#features)
- [Technology Stack](#technology-stack)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [API Documentation](#api-documentation)
- [Database Schema](#database-schema)
- [Deployment](#deployment)
- [Testing](#testing)
- [Ethical Scraping](#ethical-scraping)

## ‚ú® Features

### Core Features
- ‚úÖ **Live Web Scraping**: Real-time data extraction from World of Books using Crawlee + Playwright
- ‚úÖ **Hierarchical Navigation**: Browse from navigation headings ‚Üí categories ‚Üí products ‚Üí details
- ‚úÖ **Advanced Filtering**: Search and filter products by price, rating, author, and more
- ‚úÖ **Intelligent Caching**: TTL-based caching (24 hours default) to minimize server load
- ‚úÖ **On-Demand Refresh**: Manually trigger data updates for specific products
- ‚úÖ **Responsive Design**: Optimized for desktop, tablet, and mobile devices
- ‚úÖ **Browsing History**: Client-side and server-side history tracking
- ‚úÖ **Product Reviews**: Display user reviews and ratings
- ‚úÖ **Related Products**: Show product recommendations

### Bonus Features
- ‚úÖ **Product Search**: Full-text search across titles and authors
- ‚úÖ **Rich Filters**: Price range, minimum rating, author filters
- ‚úÖ **Pagination**: Efficient pagination for large product lists
- ‚úÖ **Loading States**: Skeleton screens for better UX
- ‚úÖ **Error Handling**: Comprehensive error handling and user feedback
- ‚úÖ **API Documentation**: Interactive Swagger/OpenAPI documentation
- ‚úÖ **Accessibility**: WCAG AA compliant with keyboard navigation

## üõ†Ô∏è Technology Stack

### Frontend
- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Data Fetching**: SWR (Stale-While-Revalidate)
- **HTTP Client**: Axios
- **Deployment**: Vercel

### Backend
- **Framework**: NestJS
- **Language**: TypeScript
- **Database**: PostgreSQL 14+
- **ORM**: TypeORM
- **Scraping**: Crawlee + Playwright
- **Documentation**: Swagger/OpenAPI
- **Deployment**: Render/Railway/Fly.io

### DevOps
- **CI/CD**: GitHub Actions
- **Version Control**: Git
- **Package Manager**: npm

## üìÅ Project Structure

```
product-data-explorer/
‚îú‚îÄ‚îÄ frontend/                 # Next.js frontend application
‚îÇ   ‚îú‚îÄ‚îÄ app/                 # App router pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx        # Home page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ products/       # Products pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ categories/     # Categories pages
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ about/          # About page
‚îÇ   ‚îú‚îÄ‚îÄ components/          # Reusable React components
‚îÇ   ‚îú‚îÄ‚îÄ lib/                # Utilities and API client
‚îÇ   ‚îî‚îÄ‚îÄ types/              # TypeScript type definitions
‚îÇ
‚îú‚îÄ‚îÄ backend/                 # NestJS backend API
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/       # TypeORM database entities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ modules/        # Feature modules
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ navigation/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ category/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ product/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ view-history/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper/        # Web scraping service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ common/         # Shared DTOs and utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.module.ts   # Main application module
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.ts         # Application entry point
‚îÇ   ‚îî‚îÄ‚îÄ test/               # Test files
‚îÇ
‚îî‚îÄ‚îÄ README.md               # This file
```

## üö¶ Getting Started

### Prerequisites

- Node.js 18+ and npm
- PostgreSQL 14+
- Git

### Installation

1. **Clone the repository**

```bash
git clone https://github.com/rooter09/webscrapping.git
cd webscrapping
```

2. **Set up the backend**

```bash
cd backend
npm install

# Copy environment variables
cp .env.example .env
# Edit .env with your database credentials

# Create PostgreSQL database
createdb product_explorer

# Run database migrations
npm run seed

# Start the backend
npm run start:dev
```

The backend will be available at `http://localhost:4000`
API Documentation: `http://localhost:4000/api`

3. **Set up the frontend**

```bash
cd frontend
npm install

# Create environment file
echo "NEXT_PUBLIC_API_URL=http://localhost:4000" > .env.local

# Start the frontend
npm run dev
```

The frontend will be available at `http://localhost:3000`

## üìö API Documentation

The API follows RESTful conventions. Full interactive documentation is available via Swagger at `/api` when the backend is running.

### Key Endpoints

#### Navigation
- `GET /navigation` - Get all navigation headings
- `GET /navigation/:id` - Get navigation by ID
- `POST /navigation/scrape` - Trigger navigation scrape

#### Categories
- `GET /categories` - Get all categories (with optional filters)
- `GET /categories/:id` - Get category details
- `POST /categories/scrape` - Trigger category scrape

#### Products
- `GET /products` - List products (with pagination & filters)
  - Query params: `page`, `limit`, `search`, `minPrice`, `maxPrice`, `minRating`, `author`, `sortBy`, `sortOrder`
- `GET /products/:id` - Get product details
- `POST /products/scrape` - Scrape products from a category
- `POST /products/:id/refresh` - Refresh product data

#### View History
- `POST /view-history` - Record a page view
- `GET /view-history/session/:sessionId` - Get session history

## üóÑÔ∏è Database Schema

### Entities

**Navigation**
- Stores top-level navigation headings from World of Books
- Fields: id, title, slug, url, lastScrapedAt

**Category**
- Hierarchical category structure with parent-child relationships
- Fields: id, navigationId, parentId, title, slug, url, productCount, lastScrapedAt

**Product**
- Core product information
- Fields: id, sourceId, categoryId, title, author, price, currency, imageUrl, sourceUrl, lastScrapedAt

**ProductDetail**
- Extended product information
- Fields: id, productId, description, specs, ratingsAvg, reviewsCount, isbn, publisher, publicationDate

**Review**
- User reviews and ratings
- Fields: id, productId, author, rating, text, title, verified, reviewDate

**ScrapeJob**
- Tracks scraping jobs and their status
- Fields: id, targetUrl, targetType, status, startedAt, finishedAt, errorLog

**ViewHistory**
- Stores user browsing history
- Fields: id, userId, sessionId, pathJson, url, productId, categoryId

## üöÄ Deployment

### Quick Start with Docker (Recommended)

The easiest way to run the entire stack locally:

```bash
# Clone the repository
git clone https://github.com/rooter09/webscrapping.git
cd webscrapping

# Copy environment file
cp .env.docker.example .env
# Edit .env with your configuration

# Start all services (PostgreSQL, Backend, Frontend)
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Access the application:
- Frontend: http://localhost:3000
- Backend API: http://localhost:4000
- API Docs: http://localhost:4000/api

### Backend Deployment (Render/Railway)

1. Create a PostgreSQL database
2. Create a new web service
3. Set environment variables:
   ```
   DATABASE_HOST=<your-db-host>
   DATABASE_PORT=5432
   DATABASE_USER=<your-db-user>
   DATABASE_PASSWORD=<your-db-password>
   DATABASE_NAME=product_explorer
   PORT=4000
   NODE_ENV=production
   CORS_ORIGIN=<your-frontend-url>
   ```
4. Deploy from GitHub repository

### Frontend Deployment (Vercel)

1. Import GitHub repository to Vercel
2. Set framework preset to Next.js
3. Add environment variable:
   ```
   NEXT_PUBLIC_API_URL=<your-backend-url>
   ```
4. Deploy

**üìñ For detailed deployment instructions, see [DEPLOYMENT.md](./DEPLOYMENT.md)**

## üß™ Testing

### Backend Tests

```bash
cd backend
npm run test           # Unit tests
npm run test:e2e       # E2E tests
npm run test:cov       # Coverage report
```

### Frontend Tests

```bash
cd frontend
npm run test           # Jest tests
npm run lint           # ESLint
```

## üîí Ethical Scraping

This project implements responsible web scraping practices:

- ‚úÖ Respects robots.txt and terms of service
- ‚úÖ Implements rate limiting (2-3 seconds between requests)
- ‚úÖ Uses exponential backoff on errors
- ‚úÖ Aggressive caching to minimize requests (24-hour TTL)
- ‚úÖ Proper error handling and logging
- ‚úÖ User-agent identification
- ‚úÖ Monitoring of scraping impact

**Note**: This project is for educational purposes. Always ensure you have permission to scrape websites and comply with their terms of service.

## üìä Performance

- Database indexing on frequently queried fields
- Pagination for all list endpoints
- Image optimization with Next.js Image component
- Lazy loading for product grids
- Efficient caching strategy
- Connection pooling for database

## ‚ôø Accessibility

- Semantic HTML5 elements
- Keyboard navigation support
- Alt text for all images
- ARIA labels where appropriate
- WCAG AA color contrast compliance
- Focus visible states

## ü§ù Contributing

This is an assignment project. For questions or issues, please contact the repository owner.

## üìÑ License

This project is created as part of a technical assignment.

## üë§ Author

- GitHub: [@rooter09](https://github.com/rooter09)

## üôè Acknowledgments

- World of Books for the data source
- NestJS and Next.js communities
- Crawlee team for the excellent scraping framework

---

**Built with ‚ù§Ô∏è using Next.js, NestJS, and TypeScript**
